"""
íŒŒì¼ ì—…ë¡œë“œ API ë¼ìš°í„°
MODIFIED 2024-01-20: íŒŒì¼ raw text ì¡°íšŒ ë° í¸ì§‘ ê¸°ëŠ¥ ì¶”ê°€
ENHANCED 2024-01-21: íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° ê¸°ëŠ¥ ì¶”ê°€
REFACTORED 2024-01-21: ì¤‘ë³µ ê²€ìƒ‰ API ì œê±° ë° ì½”ë“œ ì •ë¦¬
FIXED 2024-01-21: import ê²½ë¡œ ìˆ˜ì • (file_processing -> data_processing)
FIXED 2025-06-03: DocumentProcessor ì´ˆê¸°í™” ë° ë©”ì„œë“œ í˜¸ì¶œ ì˜¤ë¥˜ ìˆ˜ì •
"""
import os
import time
import uuid
from datetime import datetime
from pathlib import Path
from typing import List, Optional

from fastapi import APIRouter, File, Form, HTTPException, UploadFile
from fastapi.responses import JSONResponse
from pydantic import BaseModel

from database.connection import get_database
from data_processing.document_processor import DocumentProcessor
from retrieval.vector_search import VectorSearch
from utils.logger import get_logger

logger = get_logger(__name__)
router = APIRouter()

class UploadResponse(BaseModel):
    """ì—…ë¡œë“œ ì‘ë‹µ ëª¨ë¸"""
    success: bool
    message: str
    file_id: str
    original_filename: str
    processed_chunks: int
    storage_path: Optional[str] = None

class FileStatus(BaseModel):
    """íŒŒì¼ ìƒíƒœ ëª¨ë¸"""
    file_id: str
    original_filename: str
    file_type: str
    file_size: int
    status: str  # 'uploading', 'processing', 'completed', 'failed'
    processed_chunks: int
    upload_time: datetime
    folder_id: Optional[str] = None

class FileSearchRequest(BaseModel):
    """íŒŒì¼ ê²€ìƒ‰ ìš”ì²­ ëª¨ë¸"""
    query: str
    search_type: str = "both"  # filename, content, both
    folder_id: Optional[str] = None
    limit: int = 20
    skip: int = 0

class FileSearchResult(BaseModel):
    """íŒŒì¼ ê²€ìƒ‰ ê²°ê³¼ ëª¨ë¸"""
    file_id: str
    original_filename: str
    file_type: str
    file_size: int
    processed_chunks: int
    upload_time: datetime
    folder_id: Optional[str] = None
    description: Optional[str] = None
    match_type: str  # filename, content, both
    relevance_score: float
    matched_content: Optional[str] = None  # ê²€ìƒ‰ì–´ì™€ ë§¤ì¹­ëœ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°

class FileSearchResponse(BaseModel):
    """íŒŒì¼ ê²€ìƒ‰ ì‘ë‹µ ëª¨ë¸"""
    files: List[FileSearchResult]
    total_found: int
    query: str
    search_type: str
    execution_time: float

class FileUpdateRequest(BaseModel):
    """íŒŒì¼ ì •ë³´ ì—…ë°ì´íŠ¸ ìš”ì²­ ëª¨ë¸"""
    filename: Optional[str] = None
    description: Optional[str] = None
    folder_id: Optional[str] = None
    folder_title: Optional[str] = None

class FilePreviewResponse(BaseModel):
    """íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° ì‘ë‹µ ëª¨ë¸"""
    file_id: str
    original_filename: str
    file_type: str
    preview_text: str
    preview_length: int
    total_length: int
    has_more: bool
    preview_type: str  # "text", "pdf_extract", "document_extract"

@router.post("/", response_model=UploadResponse)
async def upload_file(
    file: UploadFile = File(...),
    folder_id: Optional[str] = Form(None),
    folder_title: Optional[str] = Form(None),
    description: Optional[str] = Form(None),
    preserve_formatting: bool = Form(True)  # ì¤„ë°”ê¿ˆ ë³´ì¡´ ì˜µì…˜
):
    """íŒŒì¼ ì—…ë¡œë“œ ë° ì²˜ë¦¬"""
    try:
        # ë””ë²„ê¹…: ë°›ì€ í¼ ë°ì´í„° ë¡œê·¸ ì¶œë ¥
        logger.info(f"ì—…ë¡œë“œ í¼ ë°ì´í„° - íŒŒì¼ëª…: {file.filename}, folder_id: '{folder_id}', folder_title: '{folder_title}', description: '{description}', preserve_formatting: {preserve_formatting}")
        
        # folder_idì™€ folder_title ë™ì‹œ ì…ë ¥ ë°©ì§€
        if (folder_id and folder_id.strip() and folder_id not in ["string", "null"]) and \
           (folder_title and folder_title.strip() and folder_title not in ["string", "null"]):
            raise HTTPException(
                status_code=400, 
                detail="folder_idì™€ folder_titleì€ ë™ì‹œì— ì…ë ¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•´ì£¼ì„¸ìš”."
            )
        
        # íŒŒì¼ ì •ë³´ ê²€ì¦
        if not file.filename:
            raise HTTPException(status_code=400, detail="íŒŒì¼ëª…ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ íƒ€ì… í™•ì¸
        allowed_types = ['.txt', '.pdf', '.docx', '.doc', '.md']
        file_ext = os.path.splitext(file.filename)[1].lower()
        if file_ext not in allowed_types:
            raise HTTPException(
                status_code=400, 
                detail=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤. ì§€ì› í˜•ì‹: {', '.join(allowed_types)}"
            )
        
        # íŒŒì¼ í¬ê¸° í™•ì¸ (10MB ì œí•œ)
        file_content = await file.read()
        if len(file_content) > 10 * 1024 * 1024:  # 10MB
            raise HTTPException(status_code=400, detail="íŒŒì¼ í¬ê¸°ëŠ” 10MBë¥¼ ì´ˆê³¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ì„ ë‹¤ì‹œ ì²˜ìŒìœ¼ë¡œ ë˜ëŒë¦¬ê¸°
        await file.seek(0)
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°
        db = await get_database()
        
        # íŒŒì¼ ID ìƒì„±
        file_id = str(uuid.uuid4())
        
        # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
        upload_dir = Path("uploads")
        upload_dir.mkdir(exist_ok=True)
        
        temp_filename = f"{file_id}_{file.filename}"
        temp_file_path = upload_dir / temp_filename
        
        with open(temp_file_path, "wb") as temp_file:
            temp_file.write(file_content)
        
        # Form ë°ì´í„° ì •ë¦¬ ë° í´ë” ID ê²°ì •
        clean_folder_id = None
        clean_description = None
        
        # 1. folder_id ì§ì ‘ ì…ë ¥ ì²˜ë¦¬
        if folder_id and folder_id.strip() and folder_id not in ["string", "null"]:
            clean_folder_id = folder_id.strip()
            logger.info(f"folder_idë¡œ í´ë” ì§€ì •: {clean_folder_id}")
            
            # folder_id ìœ íš¨ì„± ê²€ì¦
            from bson import ObjectId
            try:
                if not ObjectId.is_valid(clean_folder_id):
                    raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ folder_id í˜•ì‹ì…ë‹ˆë‹¤.")
                
                # í´ë” ì¡´ì¬ í™•ì¸
                folder_exists = await db.folders.find_one({"_id": ObjectId(clean_folder_id)})
                if not folder_exists:
                    raise HTTPException(status_code=404, detail=f"folder_id '{clean_folder_id}'ì— í•´ë‹¹í•˜ëŠ” í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
            except Exception as e:
                if isinstance(e, HTTPException):
                    raise
                raise HTTPException(status_code=400, detail=f"folder_id ê²€ì¦ ì‹¤íŒ¨: {str(e)}")
        
        # 2. folder_titleë¡œ í´ë” ê²€ìƒ‰ ì²˜ë¦¬
        elif folder_title and folder_title.strip() and folder_title not in ["string", "null"]:
            clean_folder_title = folder_title.strip()
            logger.info(f"folder_titleë¡œ í´ë” ê²€ìƒ‰: {clean_folder_title}")
            
            # í´ë” titleë¡œ ê²€ìƒ‰
            folder_by_title = await db.folders.find_one({"title": clean_folder_title})
            if not folder_by_title:
                raise HTTPException(
                    status_code=404, 
                    detail=f"'{clean_folder_title}' ì œëª©ì˜ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í´ë”ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”."
                )
            
            clean_folder_id = str(folder_by_title["_id"])
            logger.info(f"í´ë” title '{clean_folder_title}' -> folder_id: {clean_folder_id}")
        
        # 3. description ì²˜ë¦¬
        if description and description.strip() and description not in ["string", "null"]:
            clean_description = description.strip()
        
        logger.info(f"ìµœì¢… ì •ë¦¬ëœ ë°ì´í„° - clean_folder_id: '{clean_folder_id}', clean_description: '{clean_description}'")
        
        # íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        file_metadata = {
            "file_id": file_id,
            "original_filename": file.filename,
            "file_type": file_ext[1:],  # ì  ì œê±°
            "file_size": len(file_content),
            "upload_time": datetime.utcnow(),
            "folder_id": clean_folder_id,
            "description": clean_description
        }
        
        # ë¬¸ì„œ ì²˜ë¦¬ê¸°ë¡œ íŒŒì¼ ì²˜ë¦¬ (ì¤„ë°”ê¿ˆ ë³´ì¡´ ì˜µì…˜ ì „ë‹¬)
        processor = DocumentProcessor(db)
        result = await processor.process_and_store(
            file_path=temp_file_path,
            file_metadata=file_metadata,
            preserve_formatting=preserve_formatting
        )
        
        # ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            temp_file_path.unlink()
        except Exception as e:
            logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        # ì„±ê³µ ë©”ì‹œì§€ ìƒì„±
        format_status = "ì¤„ë°”ê¿ˆ ë³´ì¡´" if preserve_formatting else "ê¸°ë³¸ ì •ë¦¬"
        success_message = f"íŒŒì¼ ì—…ë¡œë“œê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ({format_status})"
        if clean_folder_id:
            # í´ë” ì •ë³´ ì¡°íšŒí•´ì„œ ë©”ì‹œì§€ì— í¬í•¨
            try:
                folder_info = await db.folders.find_one({"_id": ObjectId(clean_folder_id)})
                if folder_info:
                    success_message += f" (í´ë”: {folder_info['title']})"
            except:
                pass
        
        logger.info(f"íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {file.filename} -> {file_id} (í´ë”: {clean_folder_id})")
        
        return UploadResponse(
            success=True,
            message=success_message,
            file_id=file_id,
            original_filename=file.filename,
            processed_chunks=result["chunks_count"],
            storage_path=str(temp_file_path)
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.get("/status/{file_id}", response_model=FileStatus)
async def get_file_status(file_id: str):
    """íŒŒì¼ ì²˜ë¦¬ ìƒíƒœ ì¡°íšŒ"""
    try:
        db = await get_database()
        
        # 1. file_info ì»¬ë ‰ì…˜ì—ì„œ íŒŒì¼ ì •ë³´ ì¡°íšŒ (ì²˜ë¦¬ ìƒíƒœ í¬í•¨)
        file_info = await db.file_info.find_one({"file_id": file_id})
        
        if file_info:
            # file_infoì— ê¸°ë¡ì´ ìˆìœ¼ë©´ í•´ë‹¹ ìƒíƒœ ë°˜í™˜
            chunks_count = await db.chunks.count_documents({"file_id": file_id})
            
            return FileStatus(
                file_id=file_id,
                original_filename=file_info["original_filename"],
                file_type=file_info["file_type"],
                file_size=file_info["file_size"],
                status=file_info["processing_status"],  # "processing", "completed", "failed"
                processed_chunks=chunks_count,
                upload_time=file_info["upload_time"],
                folder_id=file_info.get("folder_id")
            )
        
        # 2. documents ì»¬ë ‰ì…˜ì—ì„œ íŒŒì¼ ì •ë³´ ì¡°íšŒ (ìƒˆë¡œìš´ êµ¬ì¡°)
        document = await db.documents.find_one({"file_metadata.file_id": file_id})
        
        if document:
            # documentsì— ìˆìœ¼ë©´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ëœ ê²ƒ
            chunks_count = await db.chunks.count_documents({"file_id": file_id})
            file_metadata = document["file_metadata"]
            
            return FileStatus(
                file_id=file_id,
                original_filename=file_metadata["original_filename"],
                file_type=file_metadata["file_type"],
                file_size=file_metadata["file_size"],
                status="completed",  # documentsì— ìˆìœ¼ë©´ ì²˜ë¦¬ ì™„ë£Œ
                processed_chunks=chunks_count,
                upload_time=document["created_at"],
                folder_id=document.get("folder_id")
            )
        
        # 3. chunks ì»¬ë ‰ì…˜ì—ì„œ ì§ì ‘ ì¡°íšŒ (ë ˆê±°ì‹œ í˜¸í™˜ì„±)
        chunk = await db.chunks.find_one({"file_id": file_id})
        
        if chunk:
            # chunksì—ë§Œ ìˆëŠ” ê²½ìš° (ë ˆê±°ì‹œ ë°ì´í„°)
            chunks_count = await db.chunks.count_documents({"file_id": file_id})
            metadata = chunk.get("metadata", {})
            
            return FileStatus(
                file_id=file_id,
                original_filename=metadata.get("source", "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"),
                file_type=metadata.get("file_type", "unknown"),
                file_size=0,  # chunksì—ì„œëŠ” íŒŒì¼ í¬ê¸° ì •ë³´ê°€ ì—†ìŒ
                status="completed",  # chunksì— ìˆìœ¼ë©´ ì²˜ë¦¬ ì™„ë£Œë¡œ ê°„ì£¼
                processed_chunks=chunks_count,
                upload_time=chunk.get("created_at", datetime.utcnow()),
                folder_id=chunk.get("folder_id")
            )
        
        # 4. ì–´ë””ì—ë„ ì—†ìœ¼ë©´ 404 ì—ëŸ¬
        raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/search", response_model=FileSearchResponse)
async def search_files(request: FileSearchRequest):
    """ğŸ“ ìì—°ì–´ íŒŒì¼ ê²€ìƒ‰ - íŒŒì¼ëª…ê³¼ ë‚´ìš©ìœ¼ë¡œ ê²€ìƒ‰ ê°€ëŠ¥"""
    try:
        start_time = time.time()
        db = await get_database()
        
        # 1. ê¸°ë³¸ í•„í„° ì¡°ê±´ ì„¤ì • (folder_idê°€ ì‹¤ì œ ê°’ì¼ ë•Œë§Œ ì ìš©)
        base_filter = {}
        if request.folder_id and request.folder_id.strip() and request.folder_id != "string":
            base_filter["folder_id"] = request.folder_id
        
        found_files = []
        
        # 2. íŒŒì¼ëª… ê²€ìƒ‰ (filename ë˜ëŠ” both)
        if request.search_type in ["filename", "both"]:
            filename_filter = base_filter.copy()
            # ìƒˆë¡œìš´ êµ¬ì¡°: file_metadata.original_filenameì—ì„œ ê²€ìƒ‰
            filename_filter["file_metadata.original_filename"] = {"$regex": request.query, "$options": "i"}
            
            filename_docs = await db.documents.find(filename_filter).to_list(None)
            
            for doc in filename_docs:
                file_metadata = doc.get("file_metadata", {})
                file_id = file_metadata.get("file_id")
                
                if not file_id:
                    continue
                    
                chunks_count = doc.get("chunks_count", 0)  # ì €ì¥ëœ í†µê³„ ì‚¬ìš©
                
                found_files.append({
                    "file_id": file_id,
                    "original_filename": file_metadata.get("original_filename", "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"),
                    "file_type": file_metadata.get("file_type", "unknown"),
                    "file_size": file_metadata.get("file_size", 0),
                    "processed_chunks": chunks_count,
                    "upload_time": doc.get("created_at", datetime.utcnow()),
                    "folder_id": doc.get("folder_id"),
                    "description": file_metadata.get("description"),
                    "match_type": "filename",
                    "relevance_score": 1.0,  # íŒŒì¼ëª… ë§¤ì¹˜ëŠ” ë†’ì€ ì ìˆ˜
                    "matched_content": f"íŒŒì¼ëª… ë§¤ì¹˜: {file_metadata.get('original_filename')}"
                })
        
        # 3. ë‚´ìš© ê²€ìƒ‰ (content ë˜ëŠ” both) - ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •
        if request.search_type in ["content", "both"]:
            content_filter = base_filter.copy()
            content_filter["raw_text"] = {"$regex": request.query, "$options": "i"}
            
            content_docs = await db.documents.find(content_filter).to_list(None)
            
            for doc in content_docs:
                file_metadata = doc.get("file_metadata", {})
                file_id = file_metadata.get("file_id")
                
                if not file_id:
                    continue
                
                # ì´ë¯¸ íŒŒì¼ëª…ìœ¼ë¡œ ì°¾ì€ ê²½ìš° ìŠ¤í‚µ (ì¤‘ë³µ ë°©ì§€)
                if any(f["file_id"] == file_id for f in found_files):
                    continue
                
                chunks_count = doc.get("chunks_count", 0)
                raw_text = doc.get("raw_text", "")
                
                # ë§¤ì¹­ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ê°„ë‹¨í•œ í•˜ì´ë¼ì´íŠ¸)
                query_lower = request.query.lower()
                text_lower = raw_text.lower()
                
                # ê²€ìƒ‰ì–´ ì£¼ë³€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                match_index = text_lower.find(query_lower)
                if match_index != -1:
                    start = max(0, match_index - 50)
                    end = min(len(raw_text), match_index + len(request.query) + 50)
                    matched_content = "..." + raw_text[start:end] + "..."
                else:
                    matched_content = raw_text[:100] + "..."
                
                found_files.append({
                    "file_id": file_id,
                    "original_filename": file_metadata.get("original_filename", "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"),
                    "file_type": file_metadata.get("file_type", "unknown"),
                    "file_size": file_metadata.get("file_size", 0),
                    "processed_chunks": chunks_count,
                    "upload_time": doc.get("created_at", datetime.utcnow()),
                    "folder_id": doc.get("folder_id"),
                    "description": file_metadata.get("description"),
                    "match_type": "content",
                    "relevance_score": 0.8,  # ë‚´ìš© ë§¤ì¹˜ëŠ” ì¤‘ê°„ ì ìˆ˜
                    "matched_content": matched_content
                })
        
        # 4. ê´€ë ¨ì„± ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        sorted_files = sorted(found_files, key=lambda x: x["relevance_score"], reverse=True)
        total_found = len(sorted_files)
        
        # í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©
        paginated_files = sorted_files[request.skip:request.skip + request.limit]
        
        # 5. ì‘ë‹µ ëª¨ë¸ì— ë§ê²Œ ë³€í™˜
        search_results = []
        for file_data in paginated_files:
            search_results.append(FileSearchResult(**file_data))
        
        execution_time = time.time() - start_time
        
        # ë””ë²„ê·¸ ì •ë³´ ë¡œê¹…
        logger.info(f"ê²€ìƒ‰ ì™„ë£Œ - ì¿¼ë¦¬: '{request.query}', íƒ€ì…: {request.search_type}, í´ë”: {request.folder_id}, ê²°ê³¼: {total_found}ê°œ")
        
        return FileSearchResponse(
            files=search_results,
            total_found=total_found,
            query=request.query,
            search_type=request.search_type,
            execution_time=round(execution_time, 3)
        )
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.delete("/{file_id}")
async def delete_file(file_id: str):
    """íŒŒì¼ ì™„ì „ ì‚­ì œ - ëª¨ë“  êµ¬ì¡°ë¥¼ ì§€ì›í•˜ëŠ” í†µí•© ì‚­ì œ"""
    try:
        db = await get_database()
        
        deleted_items = {
            "documents": 0,
            "chunks": 0,
            "file_info": 0,
            "labels": 0
        }
        
        logger.info(f"íŒŒì¼ ì‚­ì œ ì‹œì‘: {file_id}")
        
        # 1. documents ì»¬ë ‰ì…˜ì—ì„œ ì‚­ì œ (ìƒˆ êµ¬ì¡° + ê¸°ì¡´ êµ¬ì¡° ëª¨ë‘ ì§€ì›)
        # ìƒˆë¡œìš´ êµ¬ì¡° (file_metadata.file_id)
        doc_result_new = await db.documents.delete_many({"file_metadata.file_id": file_id})
        deleted_items["documents"] += doc_result_new.deleted_count
        
        # ê¸°ì¡´ êµ¬ì¡° (file_id ì§ì ‘)
        doc_result_old = await db.documents.delete_many({"file_id": file_id})
        deleted_items["documents"] += doc_result_old.deleted_count
        
        # 2. chunks ì»¬ë ‰ì…˜ì—ì„œ ì‚­ì œ
        chunks_result = await db.chunks.delete_many({"file_id": file_id})
        deleted_items["chunks"] = chunks_result.deleted_count
        
        # 3. file_info ì»¬ë ‰ì…˜ì—ì„œ ì‚­ì œ
        file_info_result = await db.file_info.delete_many({"file_id": file_id})
        deleted_items["file_info"] = file_info_result.deleted_count
        
        # 4. labels ì»¬ë ‰ì…˜ì—ì„œ ì‚­ì œ
        labels_result = await db.labels.delete_many({"document_id": file_id})
        deleted_items["labels"] = labels_result.deleted_count
        
        # 5. ê¸°íƒ€ ì»¬ë ‰ì…˜ë“¤ ì •ë¦¬ (ì—ëŸ¬ê°€ ë‚˜ë„ ê³„ì† ì§„í–‰)
        try:
            # summaries, qapairs, recommendationsì—ì„œ í˜¹ì‹œ file_id ì°¸ì¡° ì œê±°
            await db.summaries.delete_many({"file_id": file_id})
            await db.qapairs.delete_many({
                "$or": [
                    {"file_id": file_id},
                    {"source": file_id}
                ]
            })
            await db.recommendations.delete_many({"file_id": file_id})
        except Exception as e:
            logger.warning(f"ê¸°íƒ€ ì»¬ë ‰ì…˜ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ): {e}")
        
        # 6. ì„ì‹œ íŒŒì¼ ì‚­ì œ
        try:
            upload_dir = Path("uploads")
            deleted_files = []
            for temp_file in upload_dir.glob(f"{file_id}_*"):
                temp_file.unlink()
                deleted_files.append(str(temp_file))
            if deleted_files:
                logger.info(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ: {deleted_files}")
        except Exception as e:
            logger.warning(f"ì„ì‹œ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
        
        # 7. ê²°ê³¼ ê²€ì¦ ë° ì‘ë‹µ
        total_deleted = sum(deleted_items.values())
        
        logger.info(f"íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {file_id}")
        logger.info(f"ì‚­ì œëœ í•­ëª©ë“¤: {deleted_items}")
        
        if total_deleted == 0:
            # ì‚­ì œí•  ë°ì´í„°ê°€ ì—†ìœ¼ë©´ 404
            raise HTTPException(status_code=404, detail="ì‚­ì œí•  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        return {
            "success": True,
            "message": f"íŒŒì¼ì´ ì™„ì „íˆ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤. (ì´ {total_deleted}ê°œ í•­ëª©)",
            "file_id": file_id,
            "deleted_counts": deleted_items,
            "total_deleted": total_deleted
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.get("/list")
async def list_files(folder_id: Optional[str] = None, limit: int = 50, skip: int = 0):
    """ì—…ë¡œë“œëœ íŒŒì¼ ëª©ë¡ ì¡°íšŒ"""
    try:
        db = await get_database()
        
        # í•„í„° ì¡°ê±´
        filter_dict = {}
        if folder_id:
            filter_dict["folder_id"] = folder_id
        
        # ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ëŠ” ì •ë ¬)
        cursor = db.documents.find(filter_dict).sort("created_at", -1).skip(skip).limit(limit)
        documents = await cursor.to_list(None)
        
        # ê° ë¬¸ì„œì˜ ì²­í¬ ê°œìˆ˜ ì¡°íšŒ
        result = []
        for doc in documents:
            file_metadata = doc.get("file_metadata", {})
            file_id = file_metadata.get("file_id")
            
            if not file_id:
                continue
                
            chunks_count = await db.chunks.count_documents({"file_id": file_id})
            
            result.append({
                "file_id": file_id,
                "original_filename": file_metadata.get("original_filename", "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"),
                "file_type": file_metadata.get("file_type", "unknown"),
                "file_size": file_metadata.get("file_size", 0),
                "processed_chunks": chunks_count,
                "upload_time": doc.get("created_at", datetime.utcnow()),
                "folder_id": doc.get("folder_id"),
                "description": file_metadata.get("description")
            })
        
        return {
            "files": result,
            "total": len(result),
            "skip": skip,
            "limit": limit
        }
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/semantic-search")
async def semantic_search_files(
    q: str,  # ê²€ìƒ‰ì–´
    k: int = 5,  # ê²°ê³¼ ê°œìˆ˜
    folder_id: Optional[str] = None
):
    """AI ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰ - ë²¡í„° ìœ ì‚¬ë„ë¡œ íŒŒì¼ ì°¾ê¸°"""
    try:
        # ê²€ìƒ‰ì–´ ìœ íš¨ì„± ê²€ì‚¬
        if not q or not q.strip():
            raise HTTPException(status_code=400, detail="ê²€ìƒ‰ì–´ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        # ê²€ìƒ‰ì–´ ì •ë¦¬
        query = q.strip()
        logger.info(f"ì‹œë§¨í‹± ê²€ìƒ‰ ì‹œì‘ - ì¿¼ë¦¬: '{query}', k: {k}, folder_id: {folder_id}")
        
        db = await get_database()
        vector_search = VectorSearch(db)
        
        # í•„í„° ì¡°ê±´ ì„¤ì •
        filter_dict = {}
        if folder_id and folder_id.strip():  # Noneì´ê±°ë‚˜ ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹ ë•Œë§Œ
            filter_dict["folder_id"] = folder_id
        
        # ë²¡í„° ê²€ìƒ‰ ì‹¤í–‰
        search_results = await vector_search.search_similar(
            query=query,
            k=k * 3,  # ë” ë§ì´ ì°¾ì•„ì„œ íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”
            filter_dict=filter_dict
        )
        
        # íŒŒì¼ë³„ë¡œ ê·¸ë£¹í™”í•˜ê³  ìµœê³  ì ìˆ˜ë§Œ ë‚¨ê¸°ê¸°
        file_groups = {}
        for result in search_results:
            chunk = result.get("chunk", {})
            document = result.get("document", {})
            score = result.get("score", 0.0)
            file_id = chunk.get("file_id")
            
            if file_id and (file_id not in file_groups or score > file_groups[file_id]["relevance_score"]):
                # chunks ê°œìˆ˜ ì¡°íšŒ
                chunks_count = await db.chunks.count_documents({"file_id": file_id})
                
                file_groups[file_id] = {
                    "file_id": file_id,
                    "original_filename": document.get("original_filename", "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"),
                    "file_type": document.get("file_type", "unknown"),
                    "file_size": document.get("file_size", 0),
                    "processed_chunks": chunks_count,
                    "upload_time": document.get("upload_time"),
                    "folder_id": document.get("folder_id"),
                    "description": document.get("description"),
                    "match_type": "semantic",
                    "relevance_score": score,
                    "matched_content": chunk.get("text", "")[:200] + "..."
                }
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬í•˜ê³  ìƒìœ„ kê°œë§Œ ë°˜í™˜
        sorted_files = sorted(file_groups.values(), key=lambda x: x["relevance_score"], reverse=True)
        top_files = sorted_files[:k]
        
        # ì‘ë‹µ ìƒì„±
        search_results = [FileSearchResult(**file_data) for file_data in top_files]
        
        logger.info(f"ì‹œë§¨í‹± ê²€ìƒ‰ ì™„ë£Œ - {len(search_results)}ê°œ ê²°ê³¼")
        
        return FileSearchResponse(
            files=search_results,
            total_found=len(search_results),
            query=query,
            search_type="semantic",
            execution_time=0.0  # ì‹¤ì œ ì‹œê°„ ì¸¡ì •ì€ ìƒëµ
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì˜ë¯¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì˜ë¯¸ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")

@router.get("/content/{file_id}")
async def get_file_content(file_id: str):
    """íŒŒì¼ì˜ ì›ë³¸ í…ìŠ¤íŠ¸ ë‚´ìš© ì¡°íšŒ (í† ê¸€ í‘œì‹œìš©)"""
    try:
        db = await get_database()
        
        # documents ì»¬ë ‰ì…˜ì—ì„œ íŒŒì¼ ì •ë³´ ë° í…ìŠ¤íŠ¸ ì¡°íšŒ (ìƒˆë¡œìš´ êµ¬ì¡°)
        document = await db.documents.find_one(
            {"file_metadata.file_id": file_id},
            {"file_metadata": 1, "raw_text": 1, "created_at": 1}
        )
        
        if not document:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        file_metadata = document.get("file_metadata", {})
        
        return {
            "file_id": file_id,
            "original_filename": file_metadata.get("original_filename", "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"),
            "file_type": file_metadata.get("file_type", "unknown"),
            "upload_time": document.get("created_at", datetime.utcnow()),
            "raw_text": document.get("raw_text", ""),
            "processed_text": document.get("raw_text", ""),  # ìƒˆ êµ¬ì¡°ì—ì„œëŠ” ë™ì¼
            "text_length": len(document.get("raw_text", ""))
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ë‚´ìš© ì¡°íšŒ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.put("/{file_id}")
async def update_file_info(file_id: str, request: FileUpdateRequest):
    """íŒŒì¼ ì •ë³´ ì—…ë°ì´íŠ¸ (íŒŒì¼ëª…, ì„¤ëª…, í´ë” ë“±)"""
    try:
        db = await get_database()
        from bson import ObjectId
        
        # folder_idì™€ folder_title ë™ì‹œ ì‚¬ìš© ë°©ì§€
        if request.folder_id and request.folder_title:
            raise HTTPException(
                status_code=400, 
                detail="folder_idì™€ folder_titleì€ ë™ì‹œì— ì…ë ¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì„ íƒí•´ì£¼ì„¸ìš”."
            )
        
        # ì—…ë°ì´íŠ¸í•  í•„ë“œ ì¤€ë¹„
        update_fields = {}
        file_metadata_updates = {}
        
        if request.filename is not None and request.filename.strip():
            file_metadata_updates["file_metadata.original_filename"] = request.filename.strip()
        
        if request.description is not None:
            description_value = request.description.strip() if request.description.strip() else None
            file_metadata_updates["file_metadata.description"] = description_value
        
        # í´ë” ì²˜ë¦¬
        final_folder_id = None
        
        # folder_id ì§ì ‘ ì…ë ¥
        if request.folder_id is not None:
            folder_id_input = request.folder_id.strip() if request.folder_id.strip() else None
            
            if folder_id_input and folder_id_input not in ["string", "null"]:
                # ObjectId ìœ íš¨ì„± ê²€ì¦
                if not ObjectId.is_valid(folder_id_input):
                    raise HTTPException(status_code=400, detail="ìœ íš¨í•˜ì§€ ì•Šì€ folder_id í˜•ì‹ì…ë‹ˆë‹¤.")
                
                # í´ë” ì¡´ì¬ í™•ì¸
                folder_exists = await db.folders.find_one({"_id": ObjectId(folder_id_input)})
                if not folder_exists:
                    raise HTTPException(status_code=404, detail=f"folder_id '{folder_id_input}'ì— í•´ë‹¹í•˜ëŠ” í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
                final_folder_id = folder_id_input
            else:
                final_folder_id = None  # í´ë” í•´ì œ
            
            update_fields["folder_id"] = final_folder_id
        
        # folder_titleë¡œ í´ë” ê²€ìƒ‰
        elif request.folder_title is not None:
            folder_title_input = request.folder_title.strip() if request.folder_title.strip() else None
            
            if folder_title_input and folder_title_input not in ["string", "null"]:
                # í´ë” titleë¡œ ê²€ìƒ‰
                folder_by_title = await db.folders.find_one({"title": folder_title_input})
                if not folder_by_title:
                    raise HTTPException(
                        status_code=404, 
                        detail=f"'{folder_title_input}' ì œëª©ì˜ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                    )
                
                final_folder_id = str(folder_by_title["_id"])
                update_fields["folder_id"] = final_folder_id
                logger.info(f"í´ë” title '{folder_title_input}' -> folder_id: {final_folder_id}")
            else:
                final_folder_id = None  # í´ë” í•´ì œ
                update_fields["folder_id"] = final_folder_id
        
        # ì—…ë°ì´íŠ¸í•  ë‚´ìš©ì´ ìˆëŠ”ì§€ í™•ì¸
        all_updates = {**update_fields, **file_metadata_updates}
        if not all_updates:
            raise HTTPException(status_code=400, detail="ì—…ë°ì´íŠ¸í•  ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸ (ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
        document = await db.documents.find_one({"file_metadata.file_id": file_id})
        if not document:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë¬¸ì„œ ì—…ë°ì´íŠ¸ (ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ê²Œ ì¿¼ë¦¬ ìˆ˜ì •)
        result = await db.documents.update_one(
            {"file_metadata.file_id": file_id},
            {"$set": all_updates}
        )
        
        if result.modified_count == 0:
            logger.warning(f"íŒŒì¼ ì •ë³´ ì—…ë°ì´íŠ¸ ê²°ê³¼ ì—†ìŒ: {file_id}")
        
        # í´ë” ë³€ê²½ì‹œ chunksì˜ metadataë„ ì—…ë°ì´íŠ¸
        if "folder_id" in update_fields:
            await db.chunks.update_many(
                {"file_id": file_id},
                {"$set": {"metadata.folder_id": update_fields["folder_id"]}}
            )
        
        # ì„±ê³µ ë©”ì‹œì§€ ìƒì„±
        updated_info = []
        if request.filename:
            updated_info.append(f"íŒŒì¼ëª…: {request.filename}")
        if request.description is not None:
            updated_info.append(f"ì„¤ëª…: {request.description or '(ì œê±°)'}")
        if final_folder_id:
            try:
                folder_info = await db.folders.find_one({"_id": ObjectId(final_folder_id)})
                folder_name = folder_info["title"] if folder_info else final_folder_id
                updated_info.append(f"í´ë”: {folder_name}")
            except:
                updated_info.append(f"í´ë”: {final_folder_id}")
        elif "folder_id" in update_fields and not final_folder_id:
            updated_info.append("í´ë”: (í•´ì œ)")
        
        success_message = "íŒŒì¼ ì •ë³´ê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤."
        if updated_info:
            success_message += f" ({', '.join(updated_info)})"
        
        logger.info(f"íŒŒì¼ ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {file_id} - {all_updates}")
        
        return {
            "success": True,
            "message": success_message,
            "updated_fields": all_updates,
            "file_id": file_id
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ì •ë³´ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/preview/{file_id}")
async def get_file_preview(file_id: str, max_length: int = 500):
    """íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° - ì²˜ìŒ ëª‡ ì¤„ì˜ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜"""
    try:
        db = await get_database()
        
        # documents ì»¬ë ‰ì…˜ì—ì„œ íŒŒì¼ ì •ë³´ ì¡°íšŒ (ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
        document = await db.documents.find_one(
            {"file_metadata.file_id": file_id},
            {"file_metadata": 1, "raw_text": 1}
        )
        
        if not document:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        file_metadata = document.get("file_metadata", {})
        raw_text = document.get("raw_text", "")
        file_type = file_metadata.get("file_type", "unknown")
        total_length = len(raw_text)
        
        # ë¯¸ë¦¬ë³´ê¸° í…ìŠ¤íŠ¸ ìƒì„±
        preview_text = ""
        preview_type = "text"
        
        if raw_text:
            # í…ìŠ¤íŠ¸ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë¯¸ë¦¬ë³´ê¸° ìƒì„±
            lines = raw_text.split('\n')
            current_length = 0
            preview_lines = []
            
            # ì²« ë²ˆì§¸ ì¤„ì´ ë„ˆë¬´ ê¸¸ë©´ ë‹¨ìˆœíˆ ì˜ë¼ì„œ ì‚¬ìš©
            if lines and len(lines[0]) > max_length:
                preview_text = raw_text[:max_length] + "..."
            else:
                # ì¤„ ë‹¨ìœ„ë¡œ ì¶”ê°€
                for line in lines:
                    if current_length + len(line) + 1 > max_length:  # +1 for newline
                        break
                    preview_lines.append(line)
                    current_length += len(line) + 1
                
                preview_text = '\n'.join(preview_lines)
                
                # ë§Œì•½ preview_textê°€ ì—¬ì „íˆ ë¹„ì–´ìˆë‹¤ë©´ ê°•ì œë¡œ ì¼ë¶€ í…ìŠ¤íŠ¸ ì¶”ê°€
                if not preview_text and raw_text:
                    preview_text = raw_text[:max_length] + ("..." if len(raw_text) > max_length else "")
            
            # íŒŒì¼ íƒ€ì…ì— ë”°ë¥¸ ë¯¸ë¦¬ë³´ê¸° íƒ€ì… ê²°ì •
            if file_type == "pdf":
                preview_type = "pdf_extract"
            elif file_type in ["docx", "doc"]:
                preview_type = "document_extract"
            else:
                preview_type = "text"
        else:
            preview_text = "í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        preview_length = len(preview_text)
        has_more = total_length > preview_length
        
        return FilePreviewResponse(
            file_id=file_id,
            original_filename=file_metadata.get("original_filename", "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"),
            file_type=file_type,
            preview_text=preview_text,
            preview_length=preview_length,
            total_length=total_length,
            has_more=has_more,
            preview_type=preview_type
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/preview/chunks/{file_id}")
async def get_file_preview_with_chunks(file_id: str, chunk_count: int = 3):
    """ì²­í¬ ê¸°ë°˜ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° - ì²˜ìŒ ëª‡ ê°œ ì²­í¬ì˜ ë‚´ìš©"""
    try:
        db = await get_database()
        
        # íŒŒì¼ ê¸°ë³¸ ì •ë³´ ì¡°íšŒ (ìƒˆë¡œìš´ êµ¬ì¡°ì— ë§ê²Œ ìˆ˜ì •)
        document = await db.documents.find_one(
            {"file_metadata.file_id": file_id},
            {"file_metadata": 1}
        )
        
        if not document:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        file_metadata = document.get("file_metadata", {})
        
        # ì²˜ìŒ ëª‡ ê°œ ì²­í¬ ì¡°íšŒ
        chunks_cursor = db.chunks.find(
            {"file_id": file_id}
        ).sort("sequence", 1).limit(chunk_count)
        
        chunks = await chunks_cursor.to_list(None)
        
        if not chunks:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì˜ ì²­í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì²­í¬ë“¤ì˜ í…ìŠ¤íŠ¸ë¥¼ í•©ì³ì„œ ë¯¸ë¦¬ë³´ê¸° ìƒì„±
        preview_texts = [chunk["text"] for chunk in chunks]
        preview_text = "\n\n--- ë‹¤ìŒ ì„¹ì…˜ ---\n\n".join(preview_texts)
        
        # ì „ì²´ ì²­í¬ ìˆ˜ ì¡°íšŒ
        total_chunks = await db.chunks.count_documents({"file_id": file_id})
        
        return {
            "file_id": file_id,
            "original_filename": file_metadata.get("original_filename", "ì•Œ ìˆ˜ ì—†ëŠ” íŒŒì¼"),
            "file_type": file_metadata.get("file_type", "unknown"),
            "preview_text": preview_text,
            "preview_chunks": len(chunks),
            "total_chunks": total_chunks,
            "has_more": total_chunks > len(chunks),
            "preview_type": "chunks"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì²­í¬ ê¸°ë°˜ ë¯¸ë¦¬ë³´ê¸° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/reprocess/{file_id}")
async def reprocess_file_with_formatting(
    file_id: str,
    preserve_formatting: bool = True,
    use_original_file: bool = True
):
    """
    ê¸°ì¡´ íŒŒì¼ì„ ì¤„ë°”ê¿ˆ ë³´ì¡´ ë°©ì‹ìœ¼ë¡œ ì¬ì²˜ë¦¬
    """
    try:
        db = await get_database()
        processor = DocumentProcessor(db)
        
        if use_original_file:
            # ì›ë³¸ íŒŒì¼ì—ì„œ ì¬ì²˜ë¦¬ (ìµœê³  í’ˆì§ˆ)
            result = await processor.reprocess_document_with_formatting(
                file_id=file_id,
                preserve_formatting=preserve_formatting
            )
        else:
            # ì €ì¥ëœ raw_textì—ì„œ ì¬ì²˜ë¦¬ (ì œí•œì )
            result = await processor.reprocess_from_raw_text(
                file_id=file_id,
                preserve_formatting=preserve_formatting
            )
        
        format_status = "ì¤„ë°”ê¿ˆ ë³´ì¡´" if preserve_formatting else "ê¸°ë³¸ ì •ë¦¬"
        source_type = "ì›ë³¸ íŒŒì¼" if use_original_file else "ì €ì¥ëœ í…ìŠ¤íŠ¸"
        
        return {
            "success": True,
            "message": f"íŒŒì¼ ì¬ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ({format_status}, {source_type})",
            "file_id": file_id,
            "chunks_count": result.get("chunks_count", 0),
            "preserve_formatting": preserve_formatting,
            "source_type": source_type
        }
        
    except Exception as e:
        logger.error(f"íŒŒì¼ ì¬ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/reprocess/folder/{folder_id}")
async def reprocess_folder_files(
    folder_id: str,
    preserve_formatting: bool = True
):
    """
    í´ë” ë‚´ ëª¨ë“  íŒŒì¼ì„ ì¼ê´„ ì¬ì²˜ë¦¬
    """
    try:
        db = await get_database()
        processor = DocumentProcessor(db)
        
        result = await processor.batch_reprocess_folder(
            folder_id=folder_id,
            preserve_formatting=preserve_formatting
        )
        
        format_status = "ì¤„ë°”ê¿ˆ ë³´ì¡´" if preserve_formatting else "ê¸°ë³¸ ì •ë¦¬"
        
        return {
            "success": True,
            "message": f"í´ë” ë‚´ íŒŒì¼ ì¼ê´„ ì¬ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ({format_status})",
            "folder_id": folder_id,
            "total_files": result["total_files"],
            "success_count": result["success_count"],
            "failed_count": result["failed_count"],
            "failed_files": result["failed_files"],
            "processing_details": result["processing_details"],
            "preserve_formatting": preserve_formatting
        }
        
    except Exception as e:
        logger.error(f"í´ë” ì¼ê´„ ì¬ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/reprocess/available/{file_id}")
async def check_reprocess_availability(file_id: str):
    """
    íŒŒì¼ ì¬ì²˜ë¦¬ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
    """
    try:
        db = await get_database()
        db_ops = DatabaseOperations(db)
        
        # íŒŒì¼ ì •ë³´ ì¡°íšŒ
        file_info = await db_ops.find_one("file_info", {"file_id": file_id})
        if not file_info:
            raise HTTPException(status_code=404, detail="íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì›ë³¸ íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        original_path = file_info.get("original_path")
        has_original_file = original_path and Path(original_path).exists()
        
        # ì €ì¥ëœ í…ìŠ¤íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        doc = await db_ops.find_one("documents", {"file_metadata.file_id": file_id})
        has_stored_text = doc and doc.get("raw_text")
        
        return {
            "file_id": file_id,
            "filename": file_info["original_filename"],
            "has_original_file": has_original_file,
            "has_stored_text": bool(has_stored_text),
            "original_path": original_path,
            "can_reprocess": has_original_file or has_stored_text,
            "recommended_method": "original_file" if has_original_file else "stored_text" if has_stored_text else "none",
            "current_formatting": file_info.get("preserve_formatting", "unknown")
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ì¬ì²˜ë¦¬ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e)) 